{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44d6783",
   "metadata": {},
   "source": [
    "### Stage #1 - Building an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36366de0",
   "metadata": {},
   "source": [
    "- #### Step #1 - Data preparation and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec9e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder_name = 'llm-from-scratch'\n",
      "file_name = 'the-verdict.txt'\n",
      "\n",
      "Total number of characters in 'the-verdict.txt': 20479\n",
      "\n",
      "First 1000 characters of 'the-verdict.txt':\n",
      "\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "folder_name = 'llm-from-scratch'\n",
    "file_name = 'the-verdict.txt'\n",
    "print(f\"{folder_name = }\\n{file_name = }\\n\")\n",
    "\n",
    "# Read the text/book.\n",
    "with open(file=os.path.join(os.getcwd(), folder_name, file_name), mode='r', encoding='utf-8') as f:\n",
    "    text_raw = f.read()\n",
    "\n",
    "# Print the first 1000 characters of the text.\n",
    "n_chars = 1000\n",
    "print(f\"Total number of characters in '{file_name}': {len(text_raw)}\\n\")\n",
    "print(f\"First {n_chars} characters of '{file_name}':\\n\\n{text_raw[:1000]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe845b31",
   "metadata": {},
   "source": [
    "##### __Difference between `pattern=r'(\\s)'` and `pattern=r'\\s+'` when used with `re.split`:__\n",
    "\n",
    "__`pattern=r'\\s+'`__\n",
    "- __Meaning__: Matches one or more consecutive whitespace characters (spaces, tabs, newlines, etc.).\n",
    "- __Behavior in__ `re.split`: Splits the string at every sequence of whitespace, and does not include the whitespace in the result.\n",
    "- __Example__:\n",
    "    ```python\n",
    "    import re\n",
    "    text = \"Hello   world!\\nHow are you?\"\n",
    "    print(re.split(r'\\s+', text))\n",
    "    # Output: ['Hello', 'world!', 'How', 'are', 'you?']\n",
    "    ```\n",
    "\n",
    "__`pattern=r'(\\s)'`__\n",
    "- __Meaning__: Matches a single whitespace character, and the parentheses create a capturing group.\n",
    "- __Behavior in__ `re.split`: Splits the string at every single whitespace character, and includes the matched whitespace characters in the result as separate elements.\n",
    "- __Example__:\n",
    "    ```python\n",
    "    import re\n",
    "    text = \"Hello   world!\\nHow are you?\"\n",
    "    print(re.split(r'(\\s)', text))\n",
    "    # Output: ['Hello', ' ', '', ' ', '', ' ', 'world!', '\\n', 'How', ' ', 'are', ' ', 'you?']\n",
    "    ```\n",
    "\n",
    "__Summary Table:__\n",
    "\n",
    "| Pattern   | Splits on              | Includes whitespace in result? | Example Output                                      |\n",
    "|-----------|------------------------|--------------------------------|-----------------------------------------------------|\n",
    "| `r'\\s+'`  | Any run of whitespace  | No                             | `['Hello', 'world!', 'How', 'are', 'you?']`         |\n",
    "| `r'(\\s)'` | Each whitespace char   | Yes (as separate elements)     | `['Hello', ' ', '', ' ', '', ' ', ...]`             |\n",
    "\n",
    "__In short:__\n",
    "- Use `r'\\s+'` to split and discard whitespace.\n",
    "- Use `r'(\\s)'` to split and keep each whitespace character in the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9e278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of 1000 characters from 'the-verdict.txt':\n",
      "\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through th\n",
      "\n",
      "Pattern: r'\\s+'\n",
      "Number of tokens in the sample: 176\n",
      "First 40 tokens in the sample:\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that,', 'in', 'the', 'height', 'of', 'his', 'glory,', 'he', 'had', 'dropped', 'his', 'painting,', 'married', 'a', 'rich', 'widow,', 'and']\n",
      "\n",
      "Pattern: r'(\\s)'\n",
      "Number of tokens in the sample: 355\n",
      "First 40 tokens in the sample:\n",
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ']\n",
      "\n",
      "Pattern: r'\\s+|[,.]'\n",
      "Number of tokens in the sample: 197\n",
      "First 40 tokens in the sample:\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', '', 'in', 'the', 'height', 'of', 'his', 'glory', '', 'he', 'had', 'dropped', 'his', 'painting', '', 'married', 'a']\n",
      "\n",
      "Pattern: r'(\\s+|[,.])'\n",
      "Number of tokens in the sample: 393\n",
      "First 40 tokens in the sample:\n",
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample of text.\n",
    "n_chars = 1000\n",
    "n_tokens = 40\n",
    "text_sample = text_raw[:n_chars]\n",
    "print(f\"Sample of {n_chars} characters from '{file_name}':\\n\\n{text_sample}\\n\")\n",
    "\n",
    "# Split at 'white-space' (\\s) characters (excluding).\n",
    "pattern = r'\\s+'\n",
    "# Split the text into tokens.\n",
    "text_tokens = re.split(pattern=pattern, string=text_sample)\n",
    "print(f\"Pattern: r'{pattern}'\")\n",
    "print(f\"Number of tokens in the sample: {len(text_tokens)}\")\n",
    "print(f\"First {n_tokens} tokens in the sample:\\n{text_tokens[:n_tokens]}\\n\") # Print the first 10 tokens.\n",
    "\n",
    "# Split at 'white-space' character (including).\n",
    "pattern = r'(\\s)'\n",
    "text_tokens = re.split(pattern=pattern, string=text_sample)\n",
    "print(f\"Pattern: r'{pattern}'\")\n",
    "print(f\"Number of tokens in the sample: {len(text_tokens)}\")\n",
    "print(f\"First {n_tokens} tokens in the sample:\\n{text_tokens[:n_tokens]}\\n\")\n",
    "\n",
    "# Split at 'white-space' characters (\\s) and commas and period [,.] (excluding)\n",
    "pattern = r'\\s+|[,.]'\n",
    "text_tokens = re.split(pattern=pattern, string=text_sample)\n",
    "print(f\"Pattern: r'{pattern}'\")\n",
    "print(f\"Number of tokens in the sample: {len(text_tokens)}\")\n",
    "print(f\"First {n_tokens} tokens in the sample:\\n{text_tokens[:n_tokens]}\\n\")\n",
    "\n",
    "# Split at 'white-space' character (\\s) and commas and period [,.] (including).\n",
    "pattern = r'(\\s+|[,.])'\n",
    "text_tokens = re.split(pattern=pattern, string=text_sample)\n",
    "print(f\"Pattern: r'{pattern}'\")\n",
    "print(f\"Number of tokens in the sample: {len(text_tokens)}\")\n",
    "print(f\"First {n_tokens} tokens in the sample:\\n{text_tokens[:n_tokens]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdff1a",
   "metadata": {},
   "source": [
    "##### __`pattern = r'\\s+|([,.:;?_!\"()\\'-]|--)'`__\n",
    "\n",
    "When the above pattern is used, `None` values are generated in the split result because the regex pattern uses a capturing group.\n",
    "\n",
    "When a capturing group is used in `re.split`, the matched text for the group is included in the result. If a split occurs on the part that matches `\\s+` (whitespace), the capturing group does not match anything. Hence, `re.split` inserts `None` in the result for that split.\n",
    "\n",
    "__How to Fix?__\n",
    "After splitting, filter out `None` values from your token list.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"Hello   world! How are you? -- I'm fine.\"\n",
    "pattern = r'\\s+|([,.:;?_!\\\"()\\'-]|--)'\n",
    "tokens = re.split(pattern, text)\n",
    "# Remove None and empty strings\n",
    "tokens = [tok for tok in tokens if tok not in (None, '')]\n",
    "print(tokens)\n",
    "# Output: ['Hello', 'world', '!', 'How', 'are', 'you', '?', '--', \"I'm\", 'fine', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090b20b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern: r'\\s+|([,.:;?_!\"()\\'-]|--)'\n",
      "Number of tokens in the sample: 9341\n",
      "First 40 tokens in the sample:\n",
      "['I', None, 'HAD', None, 'always', None, 'thought', None, 'Jack', None, 'Gisburn', None, 'rather', None, 'a', None, 'cheap', None, 'genius', '-', '', '-', 'though', None, 'a', None, 'good', None, 'fellow', None, 'enough', '-', '', '-', 'so', None, 'it', None, 'was', None]\n",
      "\n",
      "Number of tokens in the sample (after cleaning): 4863\n",
      "First 40 tokens in the sample (after cleaning):\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '-', '-', 'though', 'a', 'good', 'fellow', 'enough', '-', '-', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split at 'white-space' character (\\s) (exclude them) and other special characters, like commas, period, etc. (include them).\n",
    "pattern = r'\\s+|([,.:;?_!\"()\\'-]|--)'\n",
    "text_tokens = re.split(pattern=pattern, string=text_raw)\n",
    "print(f\"Pattern: r'{pattern}'\")\n",
    "print(f\"Number of tokens in the sample: {len(text_tokens)}\")\n",
    "print(f\"First {n_tokens} tokens in the sample:\\n{text_tokens[:n_tokens]}\\n\")\n",
    "\n",
    "# Remove None and empty strings.\n",
    "text_tokens = [token for token in text_tokens if token not in (None, '')]\n",
    "print(f\"Number of tokens in the sample (after cleaning): {len(text_tokens)}\")\n",
    "print(f\"First {n_tokens} tokens in the sample (after cleaning):\\n{text_tokens[:n_tokens]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c76e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in 'unique_tokens': 1139\n",
      "\n",
      "First 20 unique tokens in 'unique_tokens':\n",
      "['!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be']\n",
      "\n",
      "First 20 items in 'vocab_token_ids':\n",
      "[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('-', 6), ('.', 7), (':', 8), (';', 9), ('?', 10), ('A', 11), ('Ah', 12), ('Among', 13), ('And', 14), ('Are', 15), ('Arrt', 16), ('As', 17), ('At', 18), ('Be', 19)]\n",
      "\n",
      "First 20 items in 'reverse_vocab_token_ids':\n",
      "[(0, '!'), (1, '\"'), (2, \"'\"), (3, '('), (4, ')'), (5, ','), (6, '-'), (7, '.'), (8, ':'), (9, ';'), (10, '?'), (11, 'A'), (12, 'Ah'), (13, 'Among'), (14, 'And'), (15, 'Are'), (16, 'Arrt'), (17, 'As'), (18, 'At'), (19, 'Be')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique tokens for the text.\n",
    "n_token_ids = 20\n",
    "unique_tokens = sorted(set(text_tokens))\n",
    "print(f\"Number of unique tokens in 'unique_tokens': {len(unique_tokens)}\\n\")\n",
    "print(f\"First {n_token_ids} unique tokens in 'unique_tokens':\\n{unique_tokens[:n_token_ids]}\\n\")\n",
    "\n",
    "# Create a dictionary of token ids.\n",
    "vocab_token_ids = {\n",
    "    token: token_id\n",
    "    for token_id, token in enumerate(unique_tokens)\n",
    "}\n",
    "print(f\"First {n_token_ids} items in 'vocab_token_ids':\\n{list(vocab_token_ids.items())[:n_token_ids]}\\n\")\n",
    "\n",
    "# Create a reverse dictionary of token ids.\n",
    "reverse_vocab_token_ids = {\n",
    "    token_id: token\n",
    "    for token, token_id in vocab_token_ids.items()\n",
    "}\n",
    "print(f\"First {n_token_ids} items in 'reverse_vocab_token_ids':\\n{list(reverse_vocab_token_ids.items())[:n_token_ids]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573d564",
   "metadata": {},
   "source": [
    "Explanation for \n",
    "```python\n",
    "text = re.sub(r'\\s+([,.:;?_!\"()\\'-]|--)', r'\\1', text)\n",
    "```\n",
    "\n",
    "- Pattern: `r'\\s+([,.:;?_!\"()\\'-]|--)'`\n",
    "\n",
    "    - `\\s+` matches one or more whitespace characters (spaces, tabs, newlines, etc.).\n",
    "    - `([,.:;?_!\"()\\'-]|--)` is a capturing group that matches any one of the listed punctuation marks or a double dash (--).\n",
    "\n",
    "- Replacement: `r'\\1'`\n",
    "    - `\\1` refers to the text matched by the capturing group (the punctuation).\n",
    "\n",
    "- What it does:\n",
    "    - __Finds__: Any whitespace that comes immediately before one of the listed punctuation marks.\n",
    "    - __Replaces__: The whitespace and the punctuation with just the punctuation (removes the whitespace before punctuation).\n",
    "\n",
    "- Example:\n",
    "    ```python\n",
    "    text = \"Hello , world ! How are you ? -- I'm fine .\"\n",
    "    text = re.sub(r'\\s+([,.:;?_!\"()\\'-]|--)', r'\\1', text)\n",
    "    print(text)\n",
    "    # Hello, world! How are you?-- I'm fine.\n",
    "    ```\n",
    "\n",
    "- Summary:\n",
    "\n",
    "    This line removes any whitespace that appears before punctuation, so punctuation is directly attached to the preceding word. This is useful for detokenizing text after splitting punctuation into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3dc625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokeniser class - version1.\n",
    "class SimpleTokeniserV1:\n",
    "    def __init__(self, vocab, pattern=r'\\s+|([,.:;?_!\"()\\'-]|--)'):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = {value: key for key, value in vocab.items()}\n",
    "        self.pattern = pattern\n",
    "    \n",
    "    def encoder(self, text):\n",
    "        tokens = re.split(pattern=self.pattern, string=text)\n",
    "        tokens = [token for token in tokens if token not in (None, '')]\n",
    "\n",
    "        token_ids = [self.vocab[token] for token in tokens]\n",
    "\n",
    "        return token_ids\n",
    "        \n",
    "    def decoder(self, token_ids):\n",
    "        text = ' '.join([self.reverse_vocab[token_id] for token_id in token_ids])\n",
    "        # _tokens = [self.reverse_vocab[_token_id] for _token_id in token_ids]\n",
    "        # _text = ' '.join(_tokens) # Join tokens with a space.\n",
    "        \n",
    "        # Remove white-space before special characters.\n",
    "        text = re.sub(''.join(self.pattern.split('|', maxsplit=1)), r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a13a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      "[1, 68, 325, 5, 903, 53, 2, 1071, 263, 752, 768, 365, 2, 979, 864, 996, 954, 118, 666, 6, 6, 1003, 864, 588, 118, 105, 42, 5, 1, 1087, 551, 735, 807, 5, 175, 535, 854, 483, 997, 980, 157, 949, 742, 736, 997, 965, 992, 7]\n",
      "\n",
      "Decoded text:\n",
      "\" My dear, since I' ve chucked painting people don' t say that stuff about me-- they say it about Victor Grindle,\" was his only protest, as he rose from the table and strolled out onto the sunlit terrace.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 'SimpleTokeniserV1' class.\n",
    "# Create a sample of text.\n",
    "text_sample = \"\"\"\n",
    "\"My dear, since I've chucked painting people don't say that stuff about me--they say it about Victor Grindle,\" was his only protest, as he rose from the table and strolled out onto the sunlit terrace.\n",
    "\"\"\"\n",
    "\n",
    "# Create a tokeniser object.\n",
    "tokeniser = SimpleTokeniserV1(vocab=vocab_token_ids)\n",
    "\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encoder(text=text_sample)\n",
    "print(f\"Encoded text:\\n{text_encoded}\\n\")\n",
    "\n",
    "# Decode the text.\n",
    "text_decoded = tokeniser.decoder(token_ids=text_encoded)\n",
    "print(f\"Decoded text:\\n{text_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ee5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in 'unique_tokens_all': 1139\n",
      "\n",
      "Number of unique tokens in 'unique_tokens_all' afterwards: 1141\n",
      "\n",
      "Last 10 items in 'vocab_token_ids_all':\n",
      "[('year', 1131), ('years', 1132), ('yellow', 1133), ('yet', 1134), ('you', 1135), ('younger', 1136), ('your', 1137), ('yourself', 1138), ('<|unk|>', 1139), ('<|endoftext|>', 1140)]\n",
      "\n",
      "Last 10 items in 'reverse_vocab_token_ids_all':\n",
      "[(1131, 'year'), (1132, 'years'), (1133, 'yellow'), (1134, 'yet'), (1135, 'you'), (1136, 'younger'), (1137, 'your'), (1138, 'yourself'), (1139, '<|unk|>'), (1140, '<|endoftext|>')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_token_ids = 10\n",
    "\n",
    "# Create unique tokens and add tokens for unknown ('<|unk|>') and end-of-text (<|endoftext|>).\n",
    "unique_tokens_all = sorted(set(text_tokens))\n",
    "print(f\"Number of unique tokens in 'unique_tokens_all': {len(unique_tokens_all)}\\n\")\n",
    "unique_tokens_all.extend(['<|unk|>', '<|endoftext|>'])\n",
    "print(f\"Number of unique tokens in 'unique_tokens_all' afterwards: {len(unique_tokens_all)}\\n\")\n",
    "\n",
    "# Create a dictionary of token ids.\n",
    "vocab_token_ids_all = {\n",
    "    token: token_id\n",
    "    for token_id, token in enumerate(unique_tokens_all)\n",
    "}\n",
    "print(f\"Last {n_token_ids} items in 'vocab_token_ids_all':\\n{list(vocab_token_ids_all.items())[-n_token_ids:]}\\n\")\n",
    "\n",
    "# Create a reverse dictionary of token ids.\n",
    "reverse_vocab_token_ids_all = {\n",
    "    token_id: token\n",
    "    for token, token_id in vocab_token_ids_all.items()\n",
    "}\n",
    "print(f\"Last {n_token_ids} items in 'reverse_vocab_token_ids_all':\\n{list(reverse_vocab_token_ids_all.items())[-n_token_ids:]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31a663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create updated tokeniser class - version2.\n",
    "class SimpleTokeniserV2:\n",
    "    def __init__(self, vocab, pattern=r'\\s+|([,.:;?_!\"()\\'-]|--)'):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = {value: key for key, value in vocab.items()}\n",
    "        self.pattern = pattern\n",
    "    \n",
    "    def encoder(self, text):\n",
    "        tokens = re.split(pattern=self.pattern, string=text)\n",
    "        tokens = [token for token in tokens if token not in (None, '')]\n",
    "\n",
    "        # Add unknown token for unknown tokens.\n",
    "        tokens = [\n",
    "            token if token in self.vocab else '<|unk|>' \n",
    "            for token in tokens\n",
    "        ]\n",
    "        token_ids = [self.vocab[token] for token in tokens]\n",
    "\n",
    "        return token_ids\n",
    "        \n",
    "    def decoder(self, token_ids):\n",
    "        text = ' '.join([self.reverse_vocab[token_id] for token_id in token_ids])\n",
    "        \n",
    "        # Remove white-space before special characters.\n",
    "        text = re.sub(''.join(self.pattern.split('|', maxsplit=1)), r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b51c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample with 'end-of-text' token:\n",
      "Hello, world...!!! <|endoftext|> Welcome to the world of LLMs.\n",
      "\n",
      "Encoded text:\n",
      "[1139, 5, 1139, 7, 7, 7, 0, 0, 0, 1140, 1139, 1025, 997, 1139, 726, 1139, 7]\n",
      "\n",
      "Decoded text:\n",
      "<|unk|>, <|unk|>...!!! <|endoftext|> <|unk|> to the <|unk|> of <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# Test 'SimpleTokeniserV2' class.\n",
    "# Create a sample of text.\n",
    "text_01 = \"\"\"Hello, world...!!!\"\"\"\n",
    "text_02 = \"\"\"Welcome to the world of LLMs.\"\"\"\n",
    "\n",
    "# Create 'end-of-text' token.\n",
    "text_sample = ' <|endoftext|> '.join((text_01, text_02))\n",
    "print(f\"Text sample with 'end-of-text' token:\\n{text_sample}\\n\")\n",
    "\n",
    "# Create a tokeniser object.\n",
    "tokeniser = SimpleTokeniserV2(vocab=vocab_token_ids_all)\n",
    "\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encoder(text=text_sample)\n",
    "print(f\"Encoded text:\\n{text_encoded}\\n\")\n",
    "\n",
    "# Decode the text.\n",
    "text_decoded = tokeniser.decoder(token_ids=text_encoded)\n",
    "print(f\"Decoded text:\\n{text_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2ad4a",
   "metadata": {},
   "source": [
    "Tokeniser - BPE (byte pair encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736b13aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importlib.metadata.version('tiktoken') = '0.9.0'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m tiktoken.get_encoding(encoding_name: \u001b[33m'str'\u001b[39m) -> \u001b[33m'Encoding'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
      "\u001b[31mFile:\u001b[39m      ~/Study/github/python-examples/llm-from-scratch/.venv/lib/python3.11/site-packages/tiktoken/registry.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "# uv add tiktoken\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(f\"{importlib.metadata.version('tiktoken') = }\\n\")\n",
    "\n",
    "?tiktoken.get_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb146e8",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e2b3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokeniser instance with 'tiktoken'.\n",
    "tokeniser = tiktoken.get_encoding(encoding_name='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "734da371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text with 'line-break':\n",
      "[198, 15496, 11, 995, 986, 10185, 19134, 284, 262, 995, 286, 27140, 10128, 13, 198]\n",
      "\n",
      "Encoded text with 'line-break' and 'end-of-text':\n",
      "[198, 15496, 11, 995, 986, 10185, 220, 50256, 19134, 284, 262, 995, 286, 27140, 10128, 220, 50256, 13, 198]\n",
      "\n",
      "Encoded text without 'line-break' and with 'end-of-text':\n",
      "[15496, 11, 995, 986, 10185, 220, 50256, 19134, 284, 262, 995, 286, 27140, 10128, 220, 50256, 13]\n"
     ]
    }
   ],
   "source": [
    "# Define a sample text.\n",
    "text_sample = \"\"\"\n",
    "Hello, world...!!! Welcome to the world of LLMs.\n",
    "\"\"\"\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encode(text=text_sample, allowed_special='all')\n",
    "print(f\"Encoded text with 'line-break':\\n{text_encoded}\\n\")\n",
    "\n",
    "text_sample = \"\"\"\n",
    "Hello, world...!!! <|endoftext|> Welcome to the world of LLMs <|endoftext|>.\n",
    "\"\"\"\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encode(text=text_sample, allowed_special='all')\n",
    "print(f\"Encoded text with 'line-break' and 'end-of-text':\\n{text_encoded}\\n\")\n",
    "\n",
    "text_sample = \"\"\"Hello, world...!!! <|endoftext|> Welcome to the world of LLMs <|endoftext|>.\"\"\"\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encode(text=text_sample, allowed_special='all')\n",
    "print(f\"Encoded text without 'line-break' and with 'end-of-text':\\n{text_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e85453f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      "[198, 15496, 11, 995, 986, 10185, 220, 50256, 19134, 284, 262, 995, 286, 27140, 10128, 13, 198]\n",
      "\n",
      "Decoded text:\n",
      "\n",
      "Hello, world...!!! <|endoftext|> Welcome to the world of LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a sample text.\n",
    "text_sample = \"\"\"\n",
    "Hello, world...!!! <|endoftext|> Welcome to the world of LLMs.\n",
    "\"\"\"\n",
    "\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encode(text=text_sample, allowed_special='all')\n",
    "print(f\"Encoded text:\\n{text_encoded}\\n\")\n",
    "\n",
    "# Decode the text.\n",
    "text_decoded = tokeniser.decode(tokens=text_encoded)\n",
    "print(f\"Decoded text:\\n{text_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300ee916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 'random' text:\n",
      "[198, 11994, 11322, 70, 28747, 767, 4089, 41060, 10563, 4, 3, 73, 85, 6592, 366, 25, 4211, 43, 5769, 28104, 198]\n",
      "\n",
      "Decoded 'random' text:\n",
      "\n",
      "hsakhgkk 798796 ^%$jvja \":>>L)(*)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define random text.\n",
    "text_sample = \"\"\"\n",
    "hsakhgkk 798796 ^%$jvja \":>>L)(*)\n",
    "\"\"\"\n",
    "\n",
    "# Encode the text.\n",
    "text_encoded = tokeniser.encode(text=text_sample, allowed_special='all')\n",
    "print(f\"Encoded 'random' text:\\n{text_encoded}\\n\")\n",
    "\n",
    "# Decode the text.\n",
    "text_decoded = tokeniser.decode(tokens=text_encoded)\n",
    "print(f\"Decoded 'random' text:\\n{text_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2414159",
   "metadata": {},
   "source": [
    "Create `input-target` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c60fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "print(text_raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc95667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of encoded text:\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536]\n",
      "\n",
      "Number of tokens in the text: 5145\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Sample of decoded text:\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Th\n",
      "\n",
      "Random sample of decoded text:\n",
      " and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Th\n"
     ]
    }
   ],
   "source": [
    "text_encoded = tokeniser.encode(text=text_raw, allowed_special='all')\n",
    "print(f\"Sample of encoded text:\\n{text_encoded[:100]}\\n\")\n",
    "print(f\"Number of tokens in the text: {len(text_encoded)}\\n\")\n",
    "\n",
    "print(f\"{'=='*50}\\n\")\n",
    "print(f\"Sample of decoded text:\\n{tokeniser.decode(tokens=text_encoded[:100])}\\n\")\n",
    "print(f\"Random sample of decoded text:\\n{tokeniser.decode(tokens=text_encoded[50:100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bfdd7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'x': [40, 367, 2885, 1464]\n",
      "'y': \t[367, 2885, 1464, 1807]\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "[40] ---> 367\n",
      "[40, 367] ---> 2885\n",
      "[40, 367, 2885] ---> 1464\n",
      "[40, 367, 2885, 1464] ---> 1807\n",
      "====================================================================================================\n",
      "\n",
      "I --->  H\n",
      "I H ---> AD\n",
      "I HAD --->  always\n",
      "I HAD always --->  thought\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # length of the input.\n",
    "\n",
    "x = text_encoded[:context_size]\n",
    "y = text_encoded[1:context_size + 1]\n",
    "\n",
    "print(f\"'x': {x}\")\n",
    "print(f\"'y': \\t{y}\\n\")\n",
    "\n",
    "print(f\"{'=='*50}\\n\")\n",
    "for i in range(1, context_size + 1):\n",
    "    x = text_encoded[:i]\n",
    "    y = text_encoded[i]\n",
    "    print(f\"{x} ---> {y}\")\n",
    "\n",
    "print(f\"{'=='*50}\\n\")\n",
    "for i in range(1, context_size + 1):\n",
    "    x = tokeniser.decode(tokens=text_encoded[:i])\n",
    "    y = tokeniser.decode(tokens=[text_encoded[i]])\n",
    "    print(f\"{x} ---> {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba0f197",
   "metadata": {},
   "source": [
    "Implement a `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8f3aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add torch torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokeniser, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenizes the entire text.\n",
    "        token_ids = tokeniser.encode(text=text, allowed_special='all')\n",
    "\n",
    "        # [i for i in range(0, (45 - 4), 4)]\n",
    "        # [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40]\n",
    "\n",
    "        # Uses a sliding window to chunk the book into overlapping sequences of max_length.\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            target_chunk = token_ids[i + 1:i + context_length + 1]\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "\n",
    "    # Returns the total number of rows in the dataset.\n",
    "    def __len__(self):\n",
    "        return(len(self.input_ids))\n",
    "    \n",
    "    # Returns a single row from the dataset.\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "335036af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "        text, batch_size=4, context_length=256,\n",
    "        stride=128, shuffle=True, num_workers=0\n",
    "):\n",
    "    # Initializes the tokenizer.\n",
    "    tokeniser = tiktoken.get_encoding(encoding_name='gpt2')\n",
    "\n",
    "    # Creates dataset.\n",
    "    dataset = GPTDatasetV1(\n",
    "        text=text, tokeniser=tokeniser,\n",
    "        context_length=context_length, stride=stride\n",
    "    )\n",
    "\n",
    "    # Creates dataloader.\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset, # dataset from which to load the data.\n",
    "        batch_size=batch_size, # number of samples per batch to load.\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers, # number of subprocesses (CPU processes) to use for data loading.\n",
    "        drop_last=True # drop the last incomplete batch, if the dataset size is not divisible by the batch size.\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "338f93a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0\n",
      "\n",
      "First batch of input ids (x):\n",
      "[tensor([40]), tensor([367]), tensor([2885]), tensor([1464])]\n",
      "\n",
      "First batch of target ids (y):\n",
      "[tensor([367]), tensor([2885]), tensor([1464]), tensor([1807])]\n",
      "\n",
      "Second batch of input ids (x):\n",
      "[tensor([367]), tensor([2885]), tensor([1464]), tensor([1807])]\n",
      "\n",
      "Second batch of target ids (y):\n",
      "[tensor([2885]), tensor([1464]), tensor([1807]), tensor([3619])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version.\n",
    "print(f\"PyTorch version: {torch.__version__}\\n\")\n",
    "\n",
    "# Read the text/book.\n",
    "with open(file=os.path.join(os.getcwd(), folder_name, file_name), mode='r', encoding='utf-8') as f:\n",
    "    text_raw = f.read()\n",
    "\n",
    "# Load the text into a DataLoader.\n",
    "dataloader = create_dataloader_v1(\n",
    "    text=text_raw, batch_size=1, context_length=4,\n",
    "    stride=1, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# Converts dataloader into a Python iterator to fetch the next entry via Pythonâ€™s built-in next() function.\n",
    "data_iter = iter(dataloader)\n",
    "# list(data_iter)\n",
    "\n",
    "# Fetch the first entry from the dataloader.\n",
    "x, y = next(data_iter)\n",
    "\n",
    "# Print the first batch.\n",
    "print(f\"First batch of input ids (x):\\n{x}\\n\")\n",
    "print(f\"First batch of target ids (y):\\n{y}\\n\")\n",
    "\n",
    "# Fetch the second entry from the dataloader.\n",
    "x, y = next(data_iter)\n",
    "\n",
    "# Print the second batch.\n",
    "print(f\"Second batch of input ids (x):\\n{x}\\n\")\n",
    "print(f\"Second batch of target ids (y):\\n{y}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04a6e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 4, stride = 1\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "First batch of input ids (x):\n",
      "[tensor([  40,  367, 2885, 1464]), tensor([ 367, 2885, 1464, 1807]), tensor([2885, 1464, 1807, 3619]), tensor([1464, 1807, 3619,  402])]\n",
      "\n",
      "First batch of target ids (y):\n",
      "[tensor([ 367, 2885, 1464, 1807]), tensor([2885, 1464, 1807, 3619]), tensor([1464, 1807, 3619,  402]), tensor([1807, 3619,  402,  271])]\n",
      "\n",
      "Second batch of input ids (x):\n",
      "[tensor([1807, 3619,  402,  271]), tensor([ 3619,   402,   271, 10899]), tensor([  402,   271, 10899,  2138]), tensor([  271, 10899,  2138,   257])]\n",
      "\n",
      "Second batch of target ids (y):\n",
      "[tensor([ 3619,   402,   271, 10899]), tensor([  402,   271, 10899,  2138]), tensor([  271, 10899,  2138,   257]), tensor([10899,  2138,   257,  7026])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define batch size.\n",
    "batch_size, stride = 4, 1\n",
    "print(f\"{batch_size = }, {stride = }\\n\")\n",
    "print(f\"{'=='*50}\\n\")\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    text=text_raw, batch_size=batch_size, context_length=4,\n",
    "    stride=stride, shuffle=False, num_workers=0\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Print the first batch.\n",
    "x, y = next(data_iter)\n",
    "print(f\"First batch of input ids (x):\\n{x}\\n\")\n",
    "print(f\"First batch of target ids (y):\\n{y}\\n\")\n",
    "\n",
    "# Print the second batch.\n",
    "x, y = next(data_iter)\n",
    "print(f\"Second batch of input ids (x):\\n{x}\\n\")\n",
    "print(f\"Second batch of target ids (y):\\n{y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245999a",
   "metadata": {},
   "source": [
    "Token/Vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0cf8a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Model path: /Users/shaz/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uv add gensim\n",
    "# from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(name='word2vec-google-news-300', return_path=True)\n",
    "print(f\"Model path: {model}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87112b84",
   "metadata": {},
   "source": [
    "- #### Step #2 - Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90806b70",
   "metadata": {},
   "source": [
    "- #### Step #3 - LLM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d981d8",
   "metadata": {},
   "source": [
    "### Stage #2 - Foundation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e2044",
   "metadata": {},
   "source": [
    "- #### Step #1 - Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a531da",
   "metadata": {},
   "source": [
    "- #### Step #2 - Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4050425",
   "metadata": {},
   "source": [
    "- #### Step #3 - Load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb3e87",
   "metadata": {},
   "source": [
    "### Stage #3 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d858ad8",
   "metadata": {},
   "source": [
    "- #### Step #1 - `TBC`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22e90",
   "metadata": {},
   "source": [
    "- #### Step #2 - `TBC`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
